{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaa3992e-c095-4bd7-9f14-60abd0740b64",
   "metadata": {},
   "source": [
    "# QEC 101 Lab 7: qLDPC Codes #\n",
    "\n",
    "\n",
    "One of the most promising classes of QEC codes are so called quantum low density parity check (qLDPC) codes. These codes are quite general and include well known codes like the surface code.  This lab will walk through the basics of classical LDPC codes, the challenges that arise when moving to qLDPC codes, and how to construct valid qLDPC codes with favorable properties.  You will eventually implement techniques from \"[Lift-Connected Surface Codes](https://arxiv.org/abs/2401.02911)\" connecting what you have leaned to state-of-the-art research.\n",
    "\n",
    "**Prerequisites:** This lab assumes you have a moderate knowledge of QEC and have completed the core QEC 101 courses (labs 1-4), especially the labs covering [stabilizers](https://github.com/NVIDIA/cuda-q-academic/blob/main/qec101/02_QEC_Stabilizers.ipynb) and [decoders](https://github.com/NVIDIA/cuda-q-academic/blob/main/qec101/04_QEC_Decoders.ipynb). \n",
    "\n",
    "The list below outlines what you'll be doing in each section of this lab:\n",
    "* 7.1 Learn the basics of classical LDPC codes and how to analyze their properties.\n",
    "* 7.2 Learn why quantum LDPC codes are challenging to construct and how to build hypergraph product (HGP) codes.\n",
    "* 7.3 Extend the HGP procedure to produce larger qLDPC codes with improved properties.\n",
    "* 7.4 Compare the quality of the codes you created using the NVIDIA BP+OSD decoder.\n",
    "\n",
    "Terminology you will use:\n",
    "* low density parity check, encoding rate, degree\n",
    "* hypergraph product\n",
    "* lifted product\n",
    "* circulants\n",
    "\n",
    "\n",
    "qLDPC codes have a number of favorable properties that make them promising for deployment within nearer term fault tolerant workflows.\n",
    "\n",
    "First run the cell below to prepare the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b29a2a16-344d-41f2-994c-60418b46bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "try:\n",
    "    import time\n",
    "    import cudaq_qec as qec\n",
    "    import galois\n",
    "    import cudaq_qec\n",
    "    import ipywidgets as widgets\n",
    "    import numpy as np\n",
    "    from IPython.display import display\n",
    "    from itertools import combinations\n",
    "    from scipy.sparse import csr_matrix\n",
    "\n",
    "\n",
    "except ImportError:\n",
    "    print(\"Tools not found, installing. Please restart your kernel after this is done.\")\n",
    "    !{sys.executable} -m pip install --upgrade pip\n",
    "    !{sys.executable} -m pip install galois\n",
    "    !{sys.executable} -m pip install cudaq-qec\n",
    "    !{sys.executable} -m pip install ipywidgets\n",
    "    print(\"\\nNew libraries have been installed. Please restart your kernel!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1f3e2-91eb-493a-9272-c54b6599d068",
   "metadata": {},
   "source": [
    "## 7.1 Classical LDPC Codes ##\n",
    "\n",
    "Robert Gallager first conceived of low density parity check (LDPC) codes in his [1960 MIT dissertation](https://dspace.mit.edu/handle/1721.1/11804) but it was underappreciated at the time and interest resurged in the 90's as other error correction codes rose in prominence. LDPC codes are now used widely in telecommunications and computer memory applications\n",
    "\n",
    "An LDPC code is a classical parity check error correction code with a sparse parity check matrix $H$. The parity check matrix is often represented by a Tanner graph, which was introduced in lab 4 on decoders. A Tanner graph is drawn with check nodes on the top row and variable nodes on the bottom. The Tanner graph for the Steane code is shown below. \n",
    "\n",
    "<img src=\"../Images/qldpc/steanetanner.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "A sparse $H$ means that each variable and check node only connects to a limited number of other nodes.  The **variable node degree** characterizes the maximum number of checks any (q)bit is involved in while the and **check node degree** characterizes the maximum number of (q)bits involved in any given check. Ideally, these two values are as small as possible to maintain low density.\n",
    "\n",
    "A second important property is the **encoding rate** ($r$).\n",
    "\n",
    "$$ r = \\frac{k}{n+c} $$\n",
    "\n",
    "Where, $k$ is the number of encoded logical bits, $n$ is the number of data bits, and $c$ is the number of check bits.  A high encoding rate is good and means that many logical bits can be encoded with a lower overhead. However, this competes with other properties like the code distance - i.e. the ability to correctly capture errors.\n",
    "\n",
    "What $k$ is depends on the number of linearly independent constraints. To determine this, perform Gaussian elimination over GF(2).  GF(2) comes from the world of abstract algebra and corresponds to a field of two elements.  Essentially, this just means integer math governed by mod 2 arithmetic.  The Gaussian elimination result can be used to determine rank($H$) which is related to $k$ by \n",
    "\n",
    "$$ k = n - \\mathrm{rank(}H\\mathrm{}) $$\n",
    "\n",
    "\n",
    "A final characteristic of a desirable LDPC code is how suited it is for decoding.  Common decoders like belief propagation (BP) can struggle when the Tanner graph has 4-cycles.  These form local loops (see image below) which can make it hard for the decoder to converge to a solution.\n",
    "\n",
    "<img src=\"../Images/qldpc/fourcycle.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "In most cases, it turns out that LDPC codes are very easy to generate. Random generation of $H$ usually produces a good LDPC code. This also provides flexibility as new codes can be generated as needed depending on the problem at hand.  Randomly generated codes also perform well and produce results close to the Shannon limit, that is the theoretical maximum of information that can pass through a noisy channel.\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px;\">\n",
    "    <h3 style=\"color: #76b900; margin-top: 0;\"> Exercise  1:</h3>\n",
    "    <p style=\"font-size: 16px; color: #333;\">\n",
    "Given the three parity check matrices below, write a function to analyze them and determine the check and variable node degrees, the encoding rate, the indices of any four cycles, and if any nodes are unchecked.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d7854b5-09ba-4717-9388-c1a7106198b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "H1:\n",
      "  variable degrees: [2, 2, 2, 2, 2, 2, 2, 1]\n",
      "  check    degrees: [3, 3, 3, 3, 3]\n",
      "  rate = 0.375  (k = 3)\n",
      "  4‑cycles:\n",
      "    vars (0,1)  rows (0,1)\n",
      "    vars (3,6)  rows (2,4)\n",
      "  all variables are checked\n",
      "\n",
      "H2:\n",
      "  variable degrees: [2, 2, 2, 2, 2, 2, 0]\n",
      "  check    degrees: [3, 3, 3, 3]\n",
      "  rate = 0.571  (k = 4)\n",
      "  no 4‑cycles\n",
      "  unchecked variables: [6]\n",
      "\n",
      "H3:\n",
      "  variable degrees: [2, 1, 2, 1, 2, 2, 2, 1, 2, 1, 2, 2, 2, 2, 1, 1]\n",
      "  check    degrees: [5, 4, 5, 4, 4, 4]\n",
      "  rate = 0.625  (k = 10)\n",
      "  no 4‑cycles\n",
      "  all variables are checked\n"
     ]
    }
   ],
   "source": [
    "H1 = np.array([\n",
    "    [1,1,0,0,1,0,0,0],\n",
    "    [1,1,0,0,0,1,0,0],\n",
    "    [0,0,1,1,0,0,1,0],\n",
    "    [0,0,1,0,1,0,0,1],\n",
    "    [0,0,0,1,0,1,1,0]], dtype=int)\n",
    "\n",
    "H2 = np.array([\n",
    "    [1,0,1,0,0,1,0],\n",
    "    [0,1,1,1,0,0,0],\n",
    "    [1,1,0,0,1,0,0],\n",
    "    [0,0,0,1,1,1,0]], dtype=int)\n",
    "\n",
    "\n",
    "H3 = np.array([\n",
    "    [1,0,0,1,0,1,0,0,0,1, 0, 0, 1, 0, 0, 0],  \n",
    "    [0,1,0,0,1,0,1,0,0,0, 1, 0, 0, 0, 0, 0],\n",
    "    [0,0,1,0,0,0,0,1,0,0, 0, 1, 0, 1, 1, 0],  \n",
    "    [1,0,0,0,1,0,0,0,1,0, 0, 0, 0, 1, 0, 0],  \n",
    "    [0,0,0,0,0,0,1,0,1,0, 0, 1, 1, 0, 0, 0],  \n",
    "    [0,0,1,0,0,1,0,0,0,0, 1, 0, 0, 0, 0, 1] \n",
    "], dtype=int)\n",
    "\n",
    "\n",
    "def degrees(H):\n",
    "    \"\"\" \n",
    "    function which computes the degrees of a parity check matrix\n",
    "    \n",
    "    Args:\n",
    "    H (np.array): parity check matrix\n",
    "\n",
    "    Returns:\n",
    "    (list): list of degrees for each variable bit\n",
    "    (list): list of degrees for each check bit\n",
    "    \"\"\"\n",
    "    \n",
    "    #TODO\n",
    "    return H.sum(axis=0), H.sum(axis=1)    # Sums the 1's vertically and then horizontally\n",
    "\n",
    "def unchecked_vars(H):\n",
    "    \"\"\" \n",
    "    function which identifies any unchecked variable bit\n",
    "    \n",
    "    Args:\n",
    "    H (np.array): parity check matrix\n",
    "\n",
    "    Returns:\n",
    "    (list): list of unchecked variable bits\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    return np.where(H.sum(0) == 0)[0]\n",
    "\n",
    "def four_cycles(H):\n",
    "    \"\"\" \n",
    "    function which identifies any four-cycles in a parity check matrix\n",
    "    \n",
    "    Args:\n",
    "    H (np.array): parity check matrix\n",
    "\n",
    "    Returns:\n",
    "    (list): list of nodes involved in a 4-cycle.\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    cycles = []\n",
    "    m, n = H.shape\n",
    "    for i, j in combinations(range(n), 2):    # variable‑node pairs\n",
    "        shared = np.where(H[:, i] & H[:, j])[0]\n",
    "        if shared.size >= 2:\n",
    "            for p, q in combinations(shared, 2):\n",
    "                cycles.append((i, j, p, q))\n",
    "    return cycles\n",
    "\n",
    "def encoding_rate(H):\n",
    "    \"\"\" \n",
    "    function which computes the encoding rate based on rank of H.\n",
    "    Note: Must use galois for GF2 field definition to ensure computation is correct\n",
    "    \n",
    "    Args:\n",
    "    H (np.array): parity check matrix\n",
    "\n",
    "    Returns:\n",
    "    (float): encoding rate\n",
    "    \"\"\"\n",
    "    GF2 = galois.GF(2)\n",
    "    Hgf2 = GF2(H)\n",
    "    n = Hgf2.shape[1]\n",
    "    rank = np.linalg.matrix_rank(Hgf2)          \n",
    "    k = n - rank\n",
    "    return k / n, k\n",
    "\n",
    "\n",
    "def analyze(H, name): \n",
    "    \"\"\" \n",
    "    Function that organizes and prints results from previous functions\n",
    "    \n",
    "    Args:\n",
    "    H (np.array): parity check matrix\n",
    "    name (str): name of the parity chexk matrix\n",
    "\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    #TODO\n",
    "    vdeg, cdeg = degrees(H)\n",
    "    R, k        = encoding_rate(H)\n",
    "    cycles      = four_cycles(H)\n",
    "    unchk       = unchecked_vars(H)\n",
    "\n",
    "    print(f'\\n{name}:')\n",
    "    print('  variable degrees:', vdeg.tolist())\n",
    "    print('  check    degrees:', cdeg.tolist())\n",
    "    print(f'  rate = {R:.3f}  (k = {k})')\n",
    "\n",
    "    if cycles:\n",
    "        print('  4‑cycles:')\n",
    "        for i, j, p, q in cycles:\n",
    "            print(f'    vars ({i},{j})  rows ({p},{q})')\n",
    "    else:\n",
    "        print('  no 4‑cycles')\n",
    "\n",
    "    if unchk.size:\n",
    "        print('  unchecked variables:', unchk.tolist())\n",
    "    else:\n",
    "        print('  all variables are checked')\n",
    "\n",
    "\n",
    "for H, name in [(H1, 'H1'), (H2, 'H2'), (H3, 'H3')]:\n",
    "    analyze(H, name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9861bfe7-5840-47ed-a963-23fe56635c80",
   "metadata": {},
   "source": [
    "## 7.2 Quantum LDPC ##\n",
    "\n",
    "qLDPC codes have many similarities to their classical counterparts, particularly with respect to terms like encoding rate and degree. Unfortunately, a major difference is that valid qLDPC codes with favorable properties cannot be produced by randomly generating parity check matrices. This is because the $Z$ and $X$ parity check matrices ($H_Z$ and $H_X$) must commute ($H_ZH^T_X=0$) for a valid CSS code that can correct both types of errors. \n",
    "\n",
    "The probability of randomly producing parity check matrices that commute is vanishingly small, let alone exhibit favorable properties. Cutting edge research focused on qLDPC codes is determined to find clever ways to produce quality parity check matrices that meet these constraints.  \n",
    "\n",
    "One particularly insightful approach is using [so called hypergraph product codes](https://arxiv.org/pdf/2401.02911). The idea is to take two \"good\" ( in this case a technical term meaning the codes distance scales as $n$) classical parity check matrices $H_1$ ($m_1\\times n_1$) and $H_2$ ($m_2\\times n_2$) and combine them in such a way that $H_Z$ and $H_X$ commute (i.e. $H_ZH_X^T=0$) and the resulting codes have a constant encoding rate and a distance that scales proportionally to the square root of the number of data qubits.\n",
    "\n",
    "The procedure works by defining the final parity check matrix $H$ as a block encoding of $H_Z$ and $H_X$. \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "0 & H_Z  \\\\\n",
    "H_X & 0\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Each quantum parity check matrix is then defined in terms of the the two classical base matrices.\n",
    "\n",
    "$$ H_X =\n",
    "\\begin{pmatrix}\n",
    "\\mathbf{1_{n_1}} \\otimes H_2 & H_1^T \\otimes \\mathbf{1_{m_2}}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$ H_Z =\n",
    "\\begin{pmatrix}\n",
    "H_1 \\otimes \\mathbf{1_{n_2}} & \\mathbf{1_{m_1}} \\otimes H_2^T\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This construction ensures that $H_Z$ and $H_X$ commute. The proof is as follows:\n",
    "\n",
    "\n",
    "$$ H_ZH_X^T  =\n",
    "\\begin{pmatrix}\n",
    "H_1 \\otimes \\mathbf{1_{n_2}} & \\mathbf{1_{m_1}} \\otimes H_2^T\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "(\\mathbf{1_{n_1}} \\otimes H_2)^T \\\\\n",
    "(H_1^T \\otimes \\mathbf{1_{m_2}})^T\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= (H_1 \\otimes \\mathbf{1_{n_2}})(\\mathbf{1_{n_1}} \\otimes H_2^T) + (\\mathbf{1_{m_1}} \\otimes H_2^T)(H_1 \\otimes \\mathbf{1_{m_2}})\n",
    "$$\n",
    "\n",
    "$$\n",
    "= H_1 \\otimes H_2^T + H_1 \\otimes H_2^T\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 2(H_1 \\otimes H_2^T) = 0\n",
    "$$\n",
    "\n",
    "It may not be clear at first why the final term equals zero.  Recall that all operations with parity check matrices occur mod 2.  So, taking any binary matrix and multiplying it by 2, will make every entry 0 or 2 = 0 mod 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e40d1e-d6f1-4cfa-a066-ad6e2141c27b",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px;\">\n",
    "    <h3 style=\"color: #76b900; margin-top: 0;\"> Exercise  2:</h3>\n",
    "    <p style=\"font-size: 16px; color: #333;\">\n",
    "Construct a hypergraph product code using a pair of three-qubit repetition code base matrices.  That is, begin with:\n",
    "\n",
    "$$H_1 =H_2 =\\begin{pmatrix}\n",
    "1&1&0\\\\\n",
    "0&1&1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Build the parity check matrices for $H_Z$ and  $H_X$ and confim they commute. Note: the `galois` package is used to define the matrices over a Galois field (GF2), which ensures modular arithmetic is baked in to your computations.  All operations can be performed just like you would with `numpy`.\n",
    "    </p>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cb9d9cb-be6c-4d0b-be6f-b0563f34a3b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First term in Hx\n",
      "[[1 1 0 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 0 0 1 1]]\n",
      "\n",
      " Second term in Hx\n",
      "[[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 1 0]\n",
      " [0 1 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n",
      "\n",
      " Full Hx\n",
      "[[1 1 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 1 0 0 0 0 1 0 1 0]\n",
      " [0 0 0 0 1 1 0 0 0 0 1 0 1]\n",
      " [0 0 0 0 0 0 1 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 1]]\n",
      "\n",
      "First term in Hz\n",
      "[[1 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 1 0]\n",
      " [0 0 0 0 0 1 0 0 1]]\n",
      "\n",
      " Second term in Hz\n",
      "[[1 0 0 0]\n",
      " [1 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 1]\n",
      " [0 0 0 1]]\n",
      "\n",
      " Full Hz\n",
      "[[1 0 0 1 0 0 0 0 0 1 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0 1 1 0 0]\n",
      " [0 0 1 0 0 1 0 0 0 0 1 0 0]\n",
      " [0 0 0 1 0 0 1 0 0 0 0 1 0]\n",
      " [0 0 0 0 1 0 0 1 0 0 0 1 1]\n",
      " [0 0 0 0 0 1 0 0 1 0 0 0 1]]\n",
      "\n",
      " Hz times HxT\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GF([[1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "     [0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0],\n",
       "     [0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "     [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0],\n",
       "     [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1],\n",
       "     [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1]], order=2),\n",
       " GF([[1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "     [0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "     [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0],\n",
       "     [0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1],\n",
       "     [0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0],\n",
       "     [0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1]], order=2))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = np.array([[1,1,0],\n",
    "              [0,1,1]])   #Using H as H1  = H2\n",
    "\n",
    "def HGP(H):\n",
    "    \"\"\" \n",
    "    Function which takes classical base parity check matricies and performs hypergraph product construction\n",
    "    \n",
    "    Args:\n",
    "    H (np.array): Base parity check matrix\n",
    "\n",
    "    Returns:\n",
    "    Hz (np.array): Hz matrix from HGP construction\n",
    "    Hx (np.array): Hx matrix from HGP construction\n",
    "    \"\"\"\n",
    "    #TODO Start\n",
    "    rows, cols = H.shape\n",
    "\n",
    "    I_rows = np.eye(rows, dtype=int)    \n",
    "    I_cols = np.eye(cols, dtype=int)\n",
    "\n",
    "    #TODO END\n",
    "   \n",
    "    # Constructs a Galois field and updates you matricies at Galois field.\n",
    "    GF2 = galois.GF(2) \n",
    "    \n",
    "    H = GF2(H)                               \n",
    "    I_rows = GF2(I_rows)\n",
    "    I_cols = GF2(I_cols)\n",
    "\n",
    "    #TODO start\n",
    "    print(\"First term in Hx\")\n",
    "    Hx_a = np.kron(I_cols, H)\n",
    "    print(Hx_a)\n",
    "    \n",
    "    print(\"\\n Second term in Hx\")\n",
    "    Hx_b = np.kron(H.T,I_rows)\n",
    "    print(Hx_b)\n",
    "    \n",
    "    print(\"\\n Full Hx\")\n",
    "    Hx = np.concatenate((Hx_a, Hx_b), axis=1)\n",
    "    print(Hx)\n",
    "    \n",
    "    print(\"\\nFirst term in Hz\")\n",
    "    Hz_a = np.kron(H, I_cols)\n",
    "    print(Hz_a)\n",
    "    \n",
    "    print(\"\\n Second term in Hz\")\n",
    "    Hz_b = np.kron(I_rows, H.T)\n",
    "    print(Hz_b)\n",
    "    \n",
    "    print(\"\\n Full Hz\")\n",
    "    Hz = np.concatenate((Hz_a, Hz_b), axis=1)\n",
    "    print(Hz)\n",
    "    \n",
    "    print(\"\\n Hz times HxT\")\n",
    "    print(Hz @ Hx.T)\n",
    "\n",
    "    return Hz, Hx\n",
    "    \n",
    "HGP(H)\n",
    "#TODO END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef0283b-69bc-43e5-ae84-46247cbaab6d",
   "metadata": {},
   "source": [
    "It turns out there is a nice visual interpretation of the hypergraph product code you just generated if the Tanner graphs form a multiplication table of sorts.  Each node of the product tanner graph that is the product of a check qubit with a check qubit or a data qubit with a data qubit produces a data qubit. If the top Tanner graph is a circle (data qubit) and the left Tanner graph a square (check qubit), the result is an $X$ stabilizer check.  Likewise, if the top Tanner graph is a square and the left Tanner graph a circle, the result is a $Z$ parity check, producing the tanner graph below. Does it look familiar?\n",
    "\n",
    "<img src=\"../Images/qldpc/hgp.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "\n",
    "Remarkably, it turns out the product of two size $l$ classical repetition codes is a [[ $(l+1)^2 + l^2$, $1$, $l+1)$]] surface code! This is a great example demonstrating how two very simple classical codes can construct a more sophisticated quantum code which obeys the required commutativity constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e5273a-901c-4bb4-a3b3-9e92bc517ad6",
   "metadata": {},
   "source": [
    "## 7.3 Generalizing HGP - Lifted Product Codes\n",
    "\n",
    "It is possible to build upon the HGP method in a more general manner where products are taken between two parity check matrices that have non-integer entries. Such an approach is called a **lifted product (LP)** as the elements of the parity check matrix are \"lifted\" to higher order elements.  LP codes can often retain the degree of checks and provide higher distance codes with a smaller qubit overhead. \n",
    "\n",
    "A LP construction still needs to ensure that $H_ZH_X^T=0$ holds as parity check matrices are modified to have non-integer elements. One way to ensure this is to replace parity check matrix elements with a commutative matrix ring, that is, a set of mathematical objects with properties that ensure multiplication of any elements commute, ensuring $H_ZH_X^T=0$ remains true (see the second term in the original proof of commutativity a few cells above). One example is $L \\times L$ **circulant** matrices which are defined as:\n",
    "\n",
    "$$ C = \\sum_{i=0}^{L-1} c_iP^{(i)} $$\n",
    "\n",
    "Where $P^{(i)}$ are cyclic permutation matrices that shift columns of the identity matrix by $i$ spaces to the right and where $c_i$ can be either 0 or 1. The notation $B_L(P^{(i)})$ indicates the binary representation of matrix size $L$. \n",
    "\n",
    "\n",
    "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px;\">\n",
    "    <h3 style=\"color: #76b900; margin-top: 0;\"> Exercise  3:</h3>\n",
    "    <p style=\"font-size: 16px; color: #333;\">\n",
    "Build the following binary matrix representations:\n",
    "\n",
    "* $B_4(P^{(2)})$\n",
    "* $B_5(P^{(1)}+P^{(2)})$\n",
    "* $B_3\\begin{pmatrix}\n",
    "I&0\\\\\n",
    "0&P^{(2)}\n",
    "\\end{pmatrix}$\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b3dee6f-82c2-44d3-aa80-01247fa19518",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO \n",
    "\n",
    "arr1 = np.array([\n",
    "    [0, 0, 1, 0],\n",
    "    [0, 0, 0, 1],\n",
    "    [1, 0, 0, 0],\n",
    "    [0, 1, 0, 0]\n",
    "])\n",
    "\n",
    "\n",
    "arr2 = np.array([\n",
    "    [0, 1, 1, 0, 0],\n",
    "    [0, 0, 1, 1, 0],\n",
    "    [0, 0, 0, 1, 1],\n",
    "    [1, 0, 0, 0, 1],\n",
    "    [1, 1, 0, 0, 0]\n",
    "])\n",
    "\n",
    "\n",
    "arr3 = np.array([\n",
    "    [1, 0, 0, 0, 0, 0],\n",
    "    [0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 1],\n",
    "    [0, 0, 0, 1, 0, 0],\n",
    "    [0, 0, 0, 0, 1, 0]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf2d23dc-5634-4072-bccf-ed4cc67ff8cc",
   "metadata": {},
   "source": [
    "A LP code is built by taking a HGP of two parity check matrices where each 1 is, in this case, a circulant matrix $C$. A LP code of size $L$ will increase the number of qubits and checks by a factor of L. This is because each entry in the parity check matrix is \"lifted\" and replaced by a set of $L$ checks and qubits. \n",
    "\n",
    "The same HGP equations from the previous section hold, but instead, the parity check matrices are denoted with a tilde to note that their elements are circulants where each entry is otherwise a 1. That is to say, $H=LP(\\tilde{H}_1,\\tilde{H}_2) = B_L(\\tilde{H})$\n",
    "\n",
    "The figure below, based on figure 2 of [Lift-Connected Surface Codes](https://arxiv.org/pdf/2401.02911), is helpful for understanding what the LP construction is doing. The overall procedure is similar.  First, the base matrices are selected and are used in the same HGP procedure you did previously to form $\\tilde{H}_Z$ and $\\tilde{H}_X$.  Then, each 1 in each parity check matrix is replaced with a circulant $C$. This simple swap takes select connections from the original Tanner graph, and lifts it to be replaced with a set of check and data qubits. \n",
    "\n",
    "<img src=\"../Images/qldpc/lifted.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>\n",
    "\n",
    "If $C$ is simply the identity matrix $I$, the resulting LP codes is the result of the HGP code duplicated trivially $L$ times.  Adding permutations to $C$ such as $I + P^{(1)}$ adds non-trivial checks between the HGP code copies. Notice the figure below (adapted figure 3 from [Lift-Connected Surface Codes](https://arxiv.org/pdf/2401.02911)) begins with the HGP that you performed earlier.  Then, The LP construction creates four copies of the surface code and interconnects them with parity checks.\n",
    "\n",
    "<img src=\"../Images/qldpc/LP_SC.png\" alt=\"Drawing\" style=\"width: 1100px;\"/>\n",
    "\n",
    "It turns out that the LP surface code is a [[52,4,4]] code whereas four copies of the surface code would be [[52,4,3]]. This means that the encoding rate is the same, but the code distance improves thanks to the LP procedure.  These sorts of clever constructions are driving qLDPC code research to continually improve code properties while maintaining the commutativity properties. \n",
    "\n",
    "Generally speaking, a LP surface code is parameterized with $l$ and $L$, where $l$ is the size of the base repetition code and $L$ is the number of copies produced by the lift procedure.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<div style=\"background-color: #f9fff0; border-left: 6px solid #76b900; padding: 15px; border-radius: 4px;\">\n",
    "    <h3 style=\"color: #76b900; margin-top: 0;\"> Exercise  4:</h3>\n",
    "    <p style=\"font-size: 16px; color: #333;\">\n",
    "Build the [[52,4,4]] ( $l =2$ and $L =4$) LP surface code by performing the following steps. First, use the base matrix below which can be conveniently split into $H_{copy}$, which produces trivial copies of the surface code and ($H_{int}$), which interacts these surface code copies.\n",
    "\n",
    "$\n",
    "\\begin{pmatrix}\n",
    "I & I + P^{(1)} & 0 & \\cdots & \\cdots &0 \\\\\n",
    "0 & I & I + P^{(1)} & \\cdots & \\cdots & 0 \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots &\\vdots \\\\\n",
    "0 & \\cdots & 0 & I & I + P^{(1)}  & 0\\\\\n",
    "0 & \\cdots & \\cdots & 0 & I &I+ P^{(1)}\n",
    "\\end{pmatrix}=\n",
    "\\begin{pmatrix}\n",
    "I & I & 0 & \\cdots & \\cdots &0 \\\\\n",
    "0 & I & I & \\cdots & \\cdots & 0 \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots &\\vdots \\\\\n",
    "0 & \\cdots & 0 & I & I  & 0\\\\\n",
    "0 & \\cdots & \\cdots & 0 & I &I\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "0 & P^{(1)} & 0 & \\cdots & \\cdots &0 \\\\\n",
    "0 & 0 & P^{(1)} & \\cdots & \\cdots & 0 \\\\\n",
    "\\vdots & \\ddots & \\ddots & \\ddots & \\vdots &\\vdots \\\\\n",
    "0 & \\cdots & 0 & 0 & P^{(1)}  & 0\\\\\n",
    "0 & \\cdots & \\cdots & 0 & 0 & P^{(1)}\n",
    "\\end{pmatrix}\n",
    "$\n",
    "\n",
    "To produce the [[52,4,4]] code, we can perform the HGP procedure twice using $\n",
    "H =\\begin{pmatrix}\n",
    "1 & 1 & 0 \\\\\n",
    "0 & 1 & 1  \\\\\n",
    "\\end{pmatrix} \n",
    "$ and $H =\\begin{pmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1  \\\\\n",
    "\\end{pmatrix} \n",
    "$. Lifting the first with $B_4(I)$ and the second with $B_4(P^{(1)})$ and summing the results will produce the parity check matrices for the [[52,4,4]] code.\n",
    "\n",
    "Modify your HGP function to lift `Hx_a` and `Hz_a` with an arbitrary $B$ and `Hx_b` and `Hz_b` with the transpose of $B$.  \n",
    "\n",
    "Then build the [[52,4,4]] code by lifting H_copy and H_int (defined below) with $B_4(I)$ and $B_4(P^{(1)})$ , respectively. Confirm that Hz and Hx of the final result commute.\n",
    "    </p>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6e53ef5-8216-4a7d-bf81-0dd51c1b77e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First term in Hx\n",
      "[[1 1 0 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 0 0]\n",
      " [0 0 0 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0]\n",
      " [0 0 0 0 0 0 0 1 1]]\n",
      "First term in Hx lifted\n",
      "[[1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1]]\n",
      "\n",
      " Second term in Hx\n",
      "[[1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [1 0 1 0]\n",
      " [0 1 0 1]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n",
      "Second term in Hx lifted\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "\n",
      " Full Lifted Hx\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "\n",
      "First term in Hz\n",
      "[[1 0 0 1 0 0 0 0 0]\n",
      " [0 1 0 0 1 0 0 0 0]\n",
      " [0 0 1 0 0 1 0 0 0]\n",
      " [0 0 0 1 0 0 1 0 0]\n",
      " [0 0 0 0 1 0 0 1 0]\n",
      " [0 0 0 0 0 1 0 0 1]]\n",
      "First term in Hz lifted\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "\n",
      " Second term in Hz\n",
      "[[1 0 0 0]\n",
      " [1 1 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 1 1]\n",
      " [0 0 0 1]]\n",
      "Second term in Hz lifted\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]]\n",
      "\n",
      " Full Hz\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "\n",
      " Hz times HxT\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "H_copy = np.array([[1,1,0],\n",
    "                   [0,1,1]])   \n",
    "\n",
    "B_I_4 = np.array([[1,0,0,0],\n",
    "                   [0,1,0,0],\n",
    "                   [0,0,1,0],\n",
    "                   [0,0,0,1]])   \n",
    "\n",
    "H_int = np.array([[0,1,0],\n",
    "                   [0,0,1]])  \n",
    "\n",
    "B_P1_4 = np.array([[0,1,0,0],\n",
    "                    [0,0,1,0],\n",
    "                    [0,0,0,1],\n",
    "                    [1,0,0,0]])   \n",
    "\n",
    "\n",
    "\n",
    "def LP(H, B):\n",
    "    \"\"\" \n",
    "    Function which perfoms lifted product construction of base matrices H with lift matrix B\n",
    "    \n",
    "    Args:\n",
    "    H (np.array): Base parity check matrix\n",
    "    B (np.array): Binary representation of lift matrix\n",
    "\n",
    "    Returns:\n",
    "    Hz (np.array): Hz matrix from HGP construction\n",
    "    Hx (np.array): Hx matrix from HGP construction\n",
    "    \"\"\"\n",
    "\n",
    "    rows, cols = H.shape\n",
    "\n",
    "    I_rows = np.eye(rows, dtype=int)\n",
    "    I_cols = np.eye(cols, dtype=int)\n",
    "   \n",
    "    GF2 = galois.GF(2) # allows mod 2 math.\n",
    "    \n",
    "    H = GF2(H)\n",
    "    I_rows = GF2(I_rows, dtype=int)\n",
    "    I_cols = GF2(I_cols, dtype=int)\n",
    "    B = GF2(B)\n",
    "\n",
    "    print(\"First term in Hx\")\n",
    "    Hx_a = np.kron(I_cols, H)\n",
    "    print(Hx_a)\n",
    "\n",
    "    print(\"First term in Hx lifted\")\n",
    "    Hx_a = np.kron(Hx_a, B)\n",
    "    print(Hx_a)\n",
    "    \n",
    "    print(\"\\n Second term in Hx\")\n",
    "    Hx_b = np.kron(H.T,I_rows)\n",
    "    print(Hx_b)\n",
    "\n",
    "    print(\"Second term in Hx lifted\")\n",
    "    Hx_b = np.kron(Hx_b, B.T)\n",
    "    print(Hx_b)\n",
    "    \n",
    "    print(\"\\n Full Lifted Hx\")\n",
    "    Hx = np.concatenate((Hx_a, Hx_b), axis=1)\n",
    "    print(Hx)\n",
    "    \n",
    "    print(\"\\nFirst term in Hz\")\n",
    "    Hz_a = np.kron(H, I_cols)\n",
    "    print(Hz_a)\n",
    "\n",
    "    print(\"First term in Hz lifted\")\n",
    "    Hz_a = np.kron(Hz_a, B)\n",
    "    print(Hz_a)\n",
    "    \n",
    "    print(\"\\n Second term in Hz\")\n",
    "    Hz_b = np.kron(I_rows, H.T)\n",
    "    print(Hz_b)\n",
    "\n",
    "    print(\"Second term in Hz lifted\")\n",
    "    Hz_b = np.kron(Hz_b, B.T)\n",
    "    print(Hz_b)\n",
    "    \n",
    "    print(\"\\n Full Hz\")\n",
    "    Hz = np.concatenate((Hz_a, Hz_b), axis=1)\n",
    "    print(Hz)\n",
    "    \n",
    "    print(\"\\n Hz times HxT\")\n",
    "    print(Hz @ Hx.T)\n",
    "\n",
    "    return Hz, Hx\n",
    "\n",
    "Hz_lifted_copy, Hx_lifted_copy = LP(H_copy, B_I_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4390072-4f06-4840-92d4-8c3af3c9af1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First term in Hx\n",
      "[[0 1 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1]]\n",
      "First term in Hx lifted\n",
      "[[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "\n",
      " Second term in Hx\n",
      "[[0 0 0 0]\n",
      " [0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n",
      "Second term in Hx lifted\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]]\n",
      "\n",
      " Full Lifted Hx\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 1 0]]\n",
      "\n",
      "First term in Hz\n",
      "[[0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 1]]\n",
      "First term in Hz lifted\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]]\n",
      "\n",
      " Second term in Hz\n",
      "[[0 0 0 0]\n",
      " [1 0 0 0]\n",
      " [0 1 0 0]\n",
      " [0 0 0 0]\n",
      " [0 0 1 0]\n",
      " [0 0 0 1]]\n",
      "Second term in Hz lifted\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]]\n",
      "\n",
      " Full Hz\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 1 0]]\n",
      "\n",
      " Hz times HxT\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "Hz_lifted_int, Hx_lifted_int = LP(H_int, B_P1_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f88736eb-349d-485a-82a7-75bf18176c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hz_lifted_total = Hz_lifted_copy + Hz_lifted_int\n",
    "Hx_lifted_total = Hx_lifted_copy + Hx_lifted_int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c737d2d-8a6d-4b48-a545-fcf373893379",
   "metadata": {},
   "source": [
    "Now, analyze the Hz and Hx parity check matrices to 1) make sure they commute and 2) confirm the degrees are as expected. Each stabilizer should act on maximum 6 qubits and each qubit should be involved in no more than 6 checks (summing Z and X checks as the full parity check matrix would be a concatenation of both $H_x$ and $H_z$.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14a7e5be-d6a6-4da0-a277-83f90d1ddada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Hz times HxT\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "\n",
      " Number of logical qubits encoded (data qubits minus checks)\n",
      "4\n",
      "\n",
      "hz lifted:\n",
      "  variable degrees: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "  check    degrees: [4, 4, 4, 4, 6, 6, 6, 6, 5, 5, 5, 5, 4, 4, 4, 4, 6, 6, 6, 6, 5, 5, 5, 5]\n",
      "  rate = 0.538  (k = 28)\n",
      "  4‑cycles:\n",
      "    vars (16,39)  rows (4,7)\n",
      "    vars (17,36)  rows (4,5)\n",
      "    vars (18,37)  rows (5,6)\n",
      "    vars (19,38)  rows (6,7)\n",
      "    vars (20,43)  rows (8,11)\n",
      "    vars (21,40)  rows (8,9)\n",
      "    vars (22,41)  rows (9,10)\n",
      "    vars (23,42)  rows (10,11)\n",
      "    vars (28,47)  rows (16,19)\n",
      "    vars (29,44)  rows (16,17)\n",
      "    vars (30,45)  rows (17,18)\n",
      "    vars (31,46)  rows (18,19)\n",
      "    vars (32,51)  rows (20,23)\n",
      "    vars (33,48)  rows (20,21)\n",
      "    vars (34,49)  rows (21,22)\n",
      "    vars (35,50)  rows (22,23)\n",
      "  all variables are checked\n",
      "\n",
      "hx lifted:\n",
      "  variable degrees: [1, 1, 1, 1, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 3, 3, 3, 3, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "  check    degrees: [4, 4, 4, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 5]\n",
      "  rate = 0.538  (k = 28)\n",
      "  4‑cycles:\n",
      "    vars (16,39)  rows (8,11)\n",
      "    vars (17,36)  rows (8,9)\n",
      "    vars (18,37)  rows (9,10)\n",
      "    vars (19,38)  rows (10,11)\n",
      "    vars (20,43)  rows (12,15)\n",
      "    vars (21,40)  rows (12,13)\n",
      "    vars (22,41)  rows (13,14)\n",
      "    vars (23,42)  rows (14,15)\n",
      "    vars (28,47)  rows (16,19)\n",
      "    vars (29,44)  rows (16,17)\n",
      "    vars (30,45)  rows (17,18)\n",
      "    vars (31,46)  rows (18,19)\n",
      "    vars (32,51)  rows (20,23)\n",
      "    vars (33,48)  rows (20,21)\n",
      "    vars (34,49)  rows (21,22)\n",
      "    vars (35,50)  rows (22,23)\n",
      "  all variables are checked\n"
     ]
    }
   ],
   "source": [
    "#Confirm Hz and Hx still commute\n",
    "print(\"\\n Hz times HxT\")\n",
    "print(Hz_lifted_total @ Hx_lifted_total.T)\n",
    "\n",
    "#Confirm [[52,4,4]] code was created\n",
    "print(\"\\n Number of logical qubits encoded (data qubits minus checks)\")\n",
    "print(Hz_lifted_total.shape[1]-2*Hz_lifted_total.shape[0])\n",
    "\n",
    "#Confirm degree of code is correct (should be 6 and 6)\n",
    "analyze(Hz_lifted_total.view(np.ndarray), 'hz lifted')\n",
    "analyze(Hx_lifted_total.view(np.ndarray), 'hx lifted')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00fa7f3-cac9-4148-a1f3-314acad888e1",
   "metadata": {},
   "source": [
    "## 7.4 Decoding with CUDA-Q QEC\n",
    "\n",
    "💻 Just a heads-up: This notebook is designed to be run on an environment with a GPU. If you don't have access to a GPU, feel free to read through the cells and explore the content without executing them. Enjoy learning! ⭐\n",
    "\n",
    "qLDPC codes are well suited for decoding with CUDA-Q's accelerated [BP+OSD decoder](https://nvidia.github.io/cudaqx/components/qec/introduction.html#pre-built-qec-decoders) found in the [CUDA-Q QEC library](https://nvidia.github.io/cudaqx/components/qec/introduction.html). If you want to learn more about BP+OSD decoding, complete lab 4 on decoding.\n",
    "\n",
    "As we have not discussed logical observables for these codes, the code below will randomly generate an error vector with a 5% chance of an error on each qubit (Only assuming bit flip errors for now). The decoder will produce a logical error if the decoder cannot identify all of the errors given the syndrome.  However, note that each of these errors might not produce a logical flip in practice, so this considers a worst case scenario.  \n",
    "\n",
    "In the cell below, run the decoder using the $H_z$ matrix you produced for the [[52,4,4]] LP surface code. Carefully read the code and see if you can spot where errors and syndromes are generated, where decoder options are specified, and how the decoder is called. Run the code and note what the logical error rate is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "22e4c57c-a3c5-488a-8d90-67f24c55ce8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1694 logical errors in 10000 shots\n",
      "Number of shots that converged with BP: 7966\n",
      "Average decoding time: 0.01 ms per shot\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "n = Hz_lifted_total.shape[1]  # number of physical qubits\n",
    "k = n - Hz_lifted_total.shape[0]  # number of logical qubits\n",
    "num_shots = 10000\n",
    "\n",
    "# Define error rates (can be uniform or varied per qubit)\n",
    "error_rate_vec = np.ones(n) * 0.05  # error rate for each qubit\n",
    "\n",
    "# Create decoder\n",
    "nv_dec_args = {\n",
    "    \"max_iterations\": 50,\n",
    "    \"error_rate_vec\": error_rate_vec,\n",
    "    \"use_sparsity\": True,\n",
    "    \"use_osd\": True,\n",
    "    \"osd_order\": 0,\n",
    "    \"osd_method\": 0,\n",
    "    \"bp_batch_size\": 1000\n",
    "}\n",
    "\n",
    "decoder = qec.get_decoder(\"nv-qldpc-decoder\", Hz_lifted_total, **nv_dec_args)\n",
    "\n",
    "# Generate random errors and decode\n",
    "decoding_time = 0\n",
    "bp_converged_flags = []\n",
    "num_logical_errors = 0\n",
    "\n",
    "# Generate batch of errors\n",
    "errors = np.random.binomial(1, error_rate_vec, (num_shots, n))\n",
    "syndromes = (Hz_lifted_total.view(np.ndarray) @ errors.T % 2).T\n",
    "\n",
    "# Batched decoding\n",
    "t0 = time.time()\n",
    "results = decoder.decode_batch(syndromes.tolist())\n",
    "t1 = time.time()\n",
    "decoding_time = t1 - t0\n",
    "\n",
    "# Process results\n",
    "for i, r in enumerate(results):\n",
    "    bp_converged_flags.append(r.converged)\n",
    "    decoded_error = np.array(r.result, dtype=np.uint8)\n",
    "    \n",
    "    # Check if error was corrected\n",
    "    if not np.array_equal(decoded_error, errors[i]):\n",
    "        num_logical_errors += 1\n",
    "\n",
    "# Print statistics\n",
    "print(f\"{num_logical_errors} logical errors in {num_shots} shots\")\n",
    "print(f\"Number of shots that converged with BP: {sum(bp_converged_flags)}\")\n",
    "print(f\"Average decoding time: {1e3 * decoding_time / num_shots:.2f} ms per shot\")\n",
    "\n",
    "# Optional: Single shot example\n",
    "single_syndrome = syndromes[0]\n",
    "bp_converged, decoded_result, *_ = decoder.decode(single_syndrome.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f995a87-3bf1-464c-95f3-ca709f520a29",
   "metadata": {},
   "source": [
    "Now, run the same code but use the `Hz_lifted_copy` parity check matrix. This code is simply four non-interacting copies of the surface code and hence a [[52,4,3]] code.  How does the logical error rate compare?  Can you see the benefit of the LP surface code as it adds one to the distance and outperforms copies of the surface code significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "576353b7-5dcc-4764-8ae6-da3762c4d3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2952 logical errors in 10000 shots\n",
      "Number of shots that converged with BP: 8396\n",
      "Average decoding time: 0.01 ms per shot\n"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "n = Hz_lifted_copy.shape[1]  # number of physical qubits\n",
    "k = n - Hz_lifted_copy.shape[0]  # number of logical qubits\n",
    "num_shots = 10000\n",
    "\n",
    "# Define error rates (can be uniform or varied per qubit)\n",
    "error_rate_vec = np.ones(n) * 0.05  # error rate for each qubit\n",
    "\n",
    "# Create decoder\n",
    "nv_dec_args = {\n",
    "    \"max_iterations\": 50,\n",
    "    \"error_rate_vec\": error_rate_vec,\n",
    "    \"use_sparsity\": True,\n",
    "    \"use_osd\": True,\n",
    "    \"osd_order\": 0,\n",
    "    \"osd_method\": 0,\n",
    "    \"bp_batch_size\": 1000\n",
    "}\n",
    "\n",
    "decoder = qec.get_decoder(\"nv-qldpc-decoder\", Hz_lifted_copy, **nv_dec_args)\n",
    "\n",
    "# Generate random errors and decode\n",
    "decoding_time = 0\n",
    "bp_converged_flags = []\n",
    "num_logical_errors = 0\n",
    "\n",
    "# Generate batch of errors\n",
    "errors = np.random.binomial(1, error_rate_vec, (num_shots, n))\n",
    "syndromes = (Hz_lifted_copy.view(np.ndarray) @ errors.T % 2).T\n",
    "\n",
    "# Batched decoding\n",
    "t0 = time.time()\n",
    "results = decoder.decode_batch(syndromes.tolist())\n",
    "t1 = time.time()\n",
    "decoding_time = t1 - t0\n",
    "\n",
    "# Process results\n",
    "for i, r in enumerate(results):\n",
    "    bp_converged_flags.append(r.converged)\n",
    "    decoded_error = np.array(r.result, dtype=np.uint8)\n",
    "    \n",
    "    # Check if error was corrected\n",
    "    if not np.array_equal(decoded_error, errors[i]):\n",
    "        num_logical_errors += 1\n",
    "\n",
    "# Print statistics\n",
    "print(f\"{num_logical_errors} logical errors in {num_shots} shots\")\n",
    "print(f\"Number of shots that converged with BP: {sum(bp_converged_flags)}\")\n",
    "print(f\"Average decoding time: {1e3 * decoding_time / num_shots:.2f} ms per shot\")\n",
    "\n",
    "# Optional: Single shot example\n",
    "single_syndrome = syndromes[0]\n",
    "bp_converged, decoded_result, *_ = decoder.decode(single_syndrome.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1e501-11fd-4943-a7dd-0908fb4f9410",
   "metadata": {},
   "source": [
    "Each choice of $l$ and $L$ will produce a different LP surface code.  It varies case by case if the LP approach is better than copies of the surface code. Examine the table below from [Lift-Connected Surface Codes](https://arxiv.org/pdf/2401.02911) where the code parameters were obtained via numerical simulation. Note, how the entries highlighted in green are cases where the LP construction results (top entry) in a code with the same overhead but a higher code distance compared to surface code copies (bottom entry).\n",
    "\n",
    "\n",
    "<img src=\"../Images/qldpc/LP_table.png\" alt=\"Drawing\" style=\"width: 1100px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ba3a34-2173-4df2-9224-09044ad68675",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You now have a foundational understanding of qLDPC codes and how they differ from their classical counterparts.  qLDPC codes are quite promising and will continue to be an active field of research.  The methods covered in this work are just a sample of the different ways to construct qLDPC code parity check matrices yet lay the groundwork for you to understand other state of the art techniques like [bivariate bicycle codes](https://arxiv.org/abs/2308.07915)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
